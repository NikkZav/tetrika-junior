import argparse
import csv
import logging
import requests
import time
from enum import StrEnum, auto
from collections import defaultdict

from bs4 import BeautifulSoup, PageElement, Tag


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ModeParser(StrEnum):
    HTML = auto()
    API = auto()


def get_wikipedia_page(url: str) -> str | None:
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        logger.error(f"Ошибка при загрузке страницы {url}: {e}")
        return None


def parse_category_page(html: str) -> tuple[list[str], str | None]:
    soup = BeautifulSoup(html, 'html.parser')
    titles: list[str] = []
    category_div: PageElement | None = soup.find('div', id='mw-pages')
    if not isinstance(category_div, Tag):
        logger.warning("Контейнер со списком страниц не найден или не является тегом")
        return titles, None

    for link in category_div.find_all('a'):
        if not isinstance(link, Tag):
            logger.debug("Пропущен элемент, не являющийся тегом")
            continue
        title = link.get('title')
        if isinstance(title, str):
            titles.append(title)
            logger.debug(f"Извлечен заголовок: {title}")
        else:
            logger.debug(f"Пропущен заголовок, так как title не строка: {title}")

    logger.debug(f"Найдено {len(titles)} заголовков на странице")
    next_page_link: PageElement | None = soup.find('a', string='Следующая страница')
    next_page_url: str | None = None
    if isinstance(next_page_link, Tag) and 'href' in next_page_link.attrs:
        href = next_page_link['href']
        if isinstance(href, str):
            next_page_url = href
        else:
            logger.warning(f"Атрибут href не является строкой: {href}")
    return titles, next_page_url


def get_category_members_api(category: str) -> list[str]:
    url = "https://ru.wikipedia.org/w/api.php"
    params: dict[str, str | int | None] = {
        "action": "query",
        "list": "categorymembers",
        "cmtitle": f"Категория:{category}",
        "cmtype": "page",
        "cmlimit": 500,
        "format": "json"
    }
    members: list[str] = []
    while True:
        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            members.extend([member['title'] for member in data['query']['categorymembers']])
            logger.debug(f"Получено {len(data['query']['categorymembers'])} заголовков через API")
            if 'continue' not in data:
                break
            params['cmcontinue'] = data['continue']['cmcontinue']
            # time.sleep(0.05)  # Минимальная задержка для API
        except requests.RequestException as e:
            logger.error(f"Ошибка при запросе к API: {e}")
            break
    logger.info(f"Всего получено {len(members)} заголовков через API")
    return members


def count_animals_by_letter(mode_parser: ModeParser) -> dict[str, int]:
    start_time = time.time()
    letter_counts: dict[str, int] = defaultdict(int)
    cyrillic_letters = set('АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ')
    processed_pages = 0

    match mode_parser:
        case ModeParser.API:
            logger.info("Использование веб-скрапинга для получения данных")
            base_url = "https://ru.wikipedia.org"
            start_url: str | None = f"{base_url}/wiki/Категория:Животные_по_алфавиту"
            current_url = start_url
            while current_url:
                logger.debug(f"Обработка страницы: {current_url}")
                html = get_wikipedia_page(current_url)
                if not html:
                    break
                titles, next_page_url = parse_category_page(html)
                for title in titles:
                    if title and title[0].upper() in cyrillic_letters:
                        letter_counts[title[0].upper()] += 1
                        processed_pages += 1
                        logger.debug(f"Подсчитан: {title}")
                current_url = f"{base_url}{next_page_url}" if next_page_url else None
                # time.sleep(0.5)  # Задержка для HTML
            logger.info(f"Обработано {processed_pages} страниц через HTML")
        case ModeParser.HTML:
            logger.info("Использование API для получения данных")
            titles = get_category_members_api("Животные_по_алфавиту")
            for title in titles:
                if title and title[0].upper() in cyrillic_letters:
                    letter_counts[title[0].upper()] += 1
                    processed_pages += 1
                    logger.debug(f"Подсчитан: {title}")
            logger.debug(f"Обработано {processed_pages} страниц через API")

    logger.info(f"Время выполнения: {time.time() - start_time:.2f} секунд")
    return letter_counts


def write_to_csv(letter_counts: dict[str, int], filename: str = "beasts.csv"):
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        for letter in sorted(letter_counts.keys()):
            writer.writerow([letter, letter_counts[letter]])
    logger.info(f"Результаты записаны в {filename}")


def main():
    parser = argparse.ArgumentParser(description="Парсинг животных с Википедии")
    parser.add_argument('--html', action='store_true', help="Использовать HTML-парсинг вместо API")
    mode_parser = ModeParser.HTML if parser.parse_args().html else ModeParser.API

    letter_counts = count_animals_by_letter(mode_parser=mode_parser)
    write_to_csv(letter_counts)


if __name__ == "__main__":
    main()
